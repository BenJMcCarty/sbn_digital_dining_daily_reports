{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Intro\" data-toc-modified-id=\"Intro-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Intro</a></span></li><li><span><a href=\"#‚≠ê-STAR-‚≠ê\" data-toc-modified-id=\"‚≠ê-STAR-‚≠ê-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>‚≠ê STAR ‚≠ê</a></span><ul class=\"toc-item\"><li><span><a href=\"#üîé-Situation-üîç\" data-toc-modified-id=\"üîé-Situation-üîç-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>üîé Situation üîç</a></span></li><li><span><a href=\"#‚ö†Ô∏è-Threat-‚ö†Ô∏è\" data-toc-modified-id=\"‚ö†Ô∏è-Threat-‚ö†Ô∏è-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>‚ö†Ô∏è Threat ‚ö†Ô∏è</a></span></li><li><span><a href=\"#‚öíÔ∏è-Action-‚öíÔ∏è\" data-toc-modified-id=\"‚öíÔ∏è-Action-‚öíÔ∏è-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>‚öíÔ∏è Action ‚öíÔ∏è</a></span></li><li><span><a href=\"#üìã-Result-üìã\" data-toc-modified-id=\"üìã-Result-üìã-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>üìã Result üìã</a></span></li></ul></li><li><span><a href=\"#üì¶-Imports\" data-toc-modified-id=\"üì¶-Imports-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>üì¶ Imports</a></span></li><li><span><a href=\"#Read-Data\" data-toc-modified-id=\"Read-Data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Read Data</a></span></li><li><span><a href=\"#üßº-Cleaning\" data-toc-modified-id=\"üßº-Cleaning-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>üßº Cleaning</a></span></li><li><span><a href=\"#üî®-Workflow-1:-Keep-All-Row-Labels\" data-toc-modified-id=\"üî®-Workflow-1:-Keep-All-Row-Labels-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>üî® Workflow 1: Keep All Row Labels</a></span><ul class=\"toc-item\"><li><span><a href=\"#Creating-Summary-for-Posting\" data-toc-modified-id=\"Creating-Summary-for-Posting-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Creating Summary for Posting</a></span></li></ul></li><li><span><a href=\"#Summarizing-the-Summary\" data-toc-modified-id=\"Summarizing-the-Summary-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Summarizing the Summary</a></span></li><li><span><a href=\"#∆í:-Workflow\" data-toc-modified-id=\"∆í:-Workflow-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>∆í: Workflow</a></span></li><li><span><a href=\"#So-Far...\" data-toc-modified-id=\"So-Far...-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>So Far...</a></span></li><li><span><a href=\"#3.15.22\" data-toc-modified-id=\"3.15.22-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>3.15.22</a></span></li><li><span><a href=\"#Generating-Alerts-for-RC:-Dupes-and-A&amp;G\" data-toc-modified-id=\"Generating-Alerts-for-RC:-Dupes-and-A&amp;G-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Generating Alerts for RC: Dupes and A&amp;G</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-Python-Logger\" data-toc-modified-id=\"Using-Python-Logger-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;</span>Using Python Logger</a></span></li></ul></li><li><span><a href=\"#Checkpoint---4.19.22\" data-toc-modified-id=\"Checkpoint---4.19.22-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Checkpoint - 4.19.22</a></span></li><li><span><a href=\"#Final-Script\" data-toc-modified-id=\"Final-Script-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Final Script</a></span></li><li><span><a href=\"#‚ö†Ô∏è-Old-Code---Raises-Error\" data-toc-modified-id=\"‚ö†Ô∏è-Old-Code---Raises-Error-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>‚ö†Ô∏è Old Code - Raises Error</a></span><ul class=\"toc-item\"><li><span><a href=\"#‚úîÔ∏è-FIXED-ERROR-‚úîÔ∏è\" data-toc-modified-id=\"‚úîÔ∏è-FIXED-ERROR-‚úîÔ∏è-14.1\"><span class=\"toc-item-num\">14.1&nbsp;&nbsp;</span>‚úîÔ∏è FIXED ERROR ‚úîÔ∏è</a></span></li><li><span><a href=\"#‚ö†Ô∏è-‚öíÔ∏è-TODO:-‚öíÔ∏è-‚ö†Ô∏è\" data-toc-modified-id=\"‚ö†Ô∏è-‚öíÔ∏è-TODO:-‚öíÔ∏è-‚ö†Ô∏è-14.2\"><span class=\"toc-item-num\">14.2&nbsp;&nbsp;</span>‚ö†Ô∏è ‚öíÔ∏è TODO: ‚öíÔ∏è ‚ö†Ô∏è</a></span></li><li><span><a href=\"#Workflow-1:-Dropping-Unused-Labels\" data-toc-modified-id=\"Workflow-1:-Dropping-Unused-Labels-14.3\"><span class=\"toc-item-num\">14.3&nbsp;&nbsp;</span>Workflow 1: Dropping Unused Labels</a></span></li><li><span><a href=\"#‚ö†Ô∏è-CONDENSE-W/-UPPER-WF-‚ö†Ô∏è\" data-toc-modified-id=\"‚ö†Ô∏è-CONDENSE-W/-UPPER-WF-‚ö†Ô∏è-14.4\"><span class=\"toc-item-num\">14.4&nbsp;&nbsp;</span>‚ö†Ô∏è CONDENSE W/ UPPER WF ‚ö†Ô∏è</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚≠ê STAR ‚≠ê"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîé Situation üîç"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "* NA uses DRR summary reports to post revenue in LS\n",
    "* I use report packet to review for DRR\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Threat ‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "* **Pro:** clearly provide total sum amounts for audit to post directly\n",
    "\n",
    "* **Cons:**\n",
    "    * \"Food\" = \"Food\" + \"Other\"\n",
    "    * Discounts are ambiguous\n",
    "        * FBC vs. Assoc vs. others\n",
    "    * *LOTS* of unused rows/labels create visual confusion\n",
    "    * Not always clear to which XAC to post revenue/tax/tips\n",
    "    * Unusual/new/unused categories are not always accounted for\n",
    "\n",
    "All issues result in manual review of all transactions during the Daily reporting process.\n",
    "\n",
    "---\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öíÔ∏è Action ‚öíÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Create workflow to:**\n",
    "\n",
    "* *Read existing DD reports*\n",
    "\n",
    "\n",
    "* *Clean the report to isolate relevant details*\n",
    "\n",
    "\n",
    "* *Report any irregularities:*\n",
    "    * Any discounts or charges?\n",
    "    * Any new/non-standard MOPs?\n",
    "    * Report exact amounts to post to each LS XAC\n",
    "        * See below\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**End Goal:**\n",
    "\n",
    "---\n",
    "\n",
    "> **Post these amounts in LightSpeed:**\n",
    "\n",
    "<!-- | |Dept | Rain 903 | IRD  | CC | Bar| \n",
    "| - | - | - | - | - | - |\n",
    "|**Sub-Dept** | |  |  |  |\n",
    "|Food | 1.00 | 1.00 | 1.00 | 1.00 |\n",
    "|Liquor | 2.00 | 2.00 | 2.00 | 2.00 \n",
    "|Tax | 3.00 | 3.00 | 3.00 | 3.00 \n",
    "|Gratuity | 4.00 | 4.00 | 4.00 | 4.00  -->\n",
    "\n",
    "|Dept| Food | Liquor | Tax  | Gratuity | \n",
    "| -  | -    | -      | -    | -        |\n",
    "| DR | 1.00 | 1.00   | 1.00 | 1.00     |\n",
    "| RS | 4.00 | 4.00   | 4.00 | 4.00     |\n",
    "| CC | 1.00 | 1.00   | 1.00 | 1.00     |\n",
    "| LB | 4.00 | 4.00   | 4.00 | 4.00     |\n",
    "\n",
    "---\n",
    "\n",
    " >**Investigate these amounts:**\n",
    " \n",
    " | Item            | Amount |\n",
    " | -               | - |\n",
    " | Discount        | 1.00 |\n",
    " | Charge          | 1.00 |\n",
    " | $2 Tip - Coupon | 1.00 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Result üìã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì¶ Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "I will **use standard data reading/wrangling/EDA packages,** such as Numpy, Pandas, Matplotlib, and Seaborn, to prepare and review my data prior to generating the final report.\n",
    "\n",
    "Additionally, I will **import my own personal EDA module,** containing a selection of hand-written functions to make the EDA process a bit easier.\n",
    "\n",
    "Finally, I will **create two particular variables to help determine the source data:**\n",
    "1. A Boolean variable controlling whether to use the reference files included in this repo (versus live/new reports).\n",
    "2. A filepath as a string datatype to import the data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:13.832390Z",
     "start_time": "2022-04-19T19:07:10.855448Z"
    }
   },
   "outputs": [],
   "source": [
    "## Importing Packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# from datetime import datetime, timedelta\n",
    "\n",
    "import bmcds.eda as eda\n",
    "\n",
    "from functions.script import *\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:13.848364Z",
     "start_time": "2022-04-19T19:07:13.833422Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s', \n",
    "                    datefmt='%d-%b-%y %H:%M:%S',filename='logs.log',filemode='a')\n",
    "\n",
    "## Adapted from http://docs.python.org/howto/logging-cookbook.html#logging-cookbook\n",
    "\n",
    "# Create logger and set level\n",
    "logger = logging.getLogger('Report_Logs')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create log file\n",
    "fh = logging.FileHandler('logs.log')\n",
    "fh.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:13.864305Z",
     "start_time": "2022-04-19T19:07:13.851341Z"
    }
   },
   "outputs": [],
   "source": [
    "## Specify file path\n",
    "\n",
    "file_path = './data/02_25_2022_pc.xls'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:13.880263Z",
     "start_time": "2022-04-19T19:07:13.868295Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## Read specific columns from Excel file\n",
    "\n",
    "# df = pd.read_excel(file_path, skiprows = 3, header = None,\n",
    "#                    usecols = 'A, B, D, F, H', \n",
    "#                    names = ['Label', 'Lounge', 'Rain 903', 'In-Room Dining',\n",
    "#                             'Coffee Corner'])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:13.960049Z",
     "start_time": "2022-04-19T19:07:13.884256Z"
    }
   },
   "outputs": [],
   "source": [
    "## Read all columns from file (incl. empty columns)\n",
    "\n",
    "df = pd.read_excel(file_path, skiprows = 2)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßº Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "I imported the data into a dataframe and immediately see that I will need to fix the alignment of my column names.\n",
    "\n",
    "Additionally, there are several \"Unnamed\" columns (representing empty, unlabeled columns generated as part of the report). Knowing those columns are empty, I will drop those as well.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:14.007921Z",
     "start_time": "2022-04-19T19:07:13.962045Z"
    }
   },
   "outputs": [],
   "source": [
    "## Shifting label names one to the right and drop extra column\n",
    "\n",
    "df.columns = ['Label', *df.columns[:-1]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:14.023878Z",
     "start_time": "2022-04-19T19:07:14.009917Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating list of columns to keep/drop\n",
    "\n",
    "col_labels = ['Label', 'Bar', 'Dining Room', 'Room Service', 'Starbucks']\n",
    "drop_cols = []\n",
    "\n",
    "for col in df.columns:\n",
    "    if col not in col_labels:\n",
    "        drop_cols.append(col)\n",
    "        \n",
    "drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:14.054796Z",
     "start_time": "2022-04-19T19:07:14.025881Z"
    }
   },
   "outputs": [],
   "source": [
    "## Dropping columns not representing an outlet\n",
    "\n",
    "df = df.drop(columns = drop_cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now that I re-labeled the columns to match the data, I see there are blank rows between each row, similar to the blank columns experienced above.\n",
    "\n",
    "Knowing that the reports do not have any rows that are either missing values or completely blank, I will drop rows with any blank values.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:14.070753Z",
     "start_time": "2022-04-19T19:07:14.057790Z"
    }
   },
   "outputs": [],
   "source": [
    "## How many rows have missing values, and how many do they contain?\n",
    "\n",
    "df.isna().sum(axis=1).value_counts()\n",
    "# df.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:14.102667Z",
     "start_time": "2022-04-19T19:07:14.071750Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Displaying only rows without any missing values\n",
    "\n",
    "df[df.isna().sum(axis=1) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:14.118625Z",
     "start_time": "2022-04-19T19:07:14.103665Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Displaying only rows missing any values\n",
    "\n",
    "# df[df.isna().sum(axis=1) > 0]\n",
    "df[df.isna().sum(axis=1) > 0].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The simple filtering above confirms my assumption that rows missing any number of missing values do not contain any relevant information. In the normal printout, these rows would be blank spaces between lines.\n",
    "\n",
    "Additionally, I noticed there are a few rows that are \"blank;\" they contain some sort of data, but they do not provide meaningful information.\n",
    "\n",
    "I will filter out rows with missing values and blank rows.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:14.134581Z",
     "start_time": "2022-04-19T19:07:14.119622Z"
    }
   },
   "outputs": [],
   "source": [
    "## Checking size of dataframe prior to dropping rows\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:14.150539Z",
     "start_time": "2022-04-19T19:07:14.135580Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Checking for blank rows\n",
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:14.166498Z",
     "start_time": "2022-04-19T19:07:14.152534Z"
    }
   },
   "outputs": [],
   "source": [
    "## Reviewing rows containing a blank space for a label\n",
    "df[df.loc[:, \"Label\"].isin([' '])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:14.198412Z",
     "start_time": "2022-04-19T19:07:14.169490Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Filter out those rows with missing/blank values\n",
    "\n",
    "df_cleaned = df[df.isna().sum(axis=1) == 0]\n",
    "df_cleaned = df_cleaned[~df_cleaned.loc[:, \"Label\"].isin([' '])]\n",
    "df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:14.214369Z",
     "start_time": "2022-04-19T19:07:14.200408Z"
    }
   },
   "outputs": [],
   "source": [
    "## Checking number of rows post-drop\n",
    "\n",
    "df_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:14.246283Z",
     "start_time": "2022-04-19T19:07:14.217361Z"
    }
   },
   "outputs": [],
   "source": [
    "## Inspecting datatype and remaining data\n",
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Review Current Status**\n",
    "\n",
    "At this point, I completed the following:\n",
    "\n",
    "- [X] Imported specified report data\n",
    "- [X] Removed blank columns and rows\n",
    "- [X] Removed any rows missing values\n",
    "\n",
    "At this point, I condensed the report and removed any extra spacing.\n",
    "\n",
    "**Now I need to decide which rows contain relevant data to keep and which rows contain extra data that we don't use.**\n",
    "\n",
    "---\n",
    "\n",
    "**Keeping vs. Dropping**\n",
    "\n",
    "My original intention for this notebook/eventual script was to create a very minimalistic summary of the relevant day-to-day data, cutting out the extra stuff that we ignore.\n",
    "- This would provide me with the data specifically used during our night audit processes and remove any confusing extra details.\n",
    "\n",
    "However, I realized that approach would cause problems if/when we introduce new payment methods or would have any rare situations where relevant data would come through to the reports. \n",
    "- Missing such data could result in problems creating the daily reports or worse - obscure potential problems/irregularities.\n",
    "\n",
    "---\n",
    "\n",
    "**Final Decision:**\n",
    "\n",
    "I will focus my code on keeping all row labels while maintaining a focus on the most important day-to-day details.\n",
    "- To address the concern about irregularities, I will create warnings to highlight such data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî® Workflow 1: Keep All Row Labels\n",
    "\n",
    "---\n",
    "\n",
    "To create my ideal report, **I will focus on specific rows that I use most often for my reporting.** \n",
    "\n",
    "I will split the remaining rows into two groups:\n",
    "1. Regularly-occurring (but irrelevant) data\n",
    "    - This includes subtotals and most of the rows at the end\n",
    "    - These rows are irrelevant for the day-to-day reports\n",
    "\n",
    "\n",
    "2. Infrequent but highly valuable data\n",
    "    - This group would also act as a catch-all for any new rows, such as when we create a new payment method.\n",
    "\n",
    "**I will sub-divide the dataframe into the three new dataframes, one for each group.**\n",
    "\n",
    "- Both the \"regular\" data and \"infrequent-but-valuable\" dataframes will be included on the first sheet of the final Excel workbook\n",
    "- The extra dataframe will be saved to the second page.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:14.277200Z",
     "start_time": "2022-04-19T19:07:14.248280Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Create new DF\n",
    "\n",
    "df_all_cols = df_cleaned.copy().T\n",
    "df_all_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:14.309115Z",
     "start_time": "2022-04-19T19:07:14.278198Z"
    }
   },
   "outputs": [],
   "source": [
    "## Re-labeling columns based on \"Label\" row\n",
    "df_all_cols.columns = df_all_cols.iloc[0]\n",
    "df_all_cols = df_all_cols.drop(index = 'Label')\n",
    "df_all_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:14.325075Z",
     "start_time": "2022-04-19T19:07:14.310113Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Specifying daily-use labels\n",
    "\n",
    "daily_rows = ['Food', 'Beverage', 'Other', 'Discount', 'MD Food 6%',\n",
    "              'MD Liq 9%', 'Tip collected', \"ALL A&G CHRG\", 'Room Charge',\n",
    "              'American Express', 'Cash', 'Discover Card', 'MasterCard', 'Visa']\n",
    "\n",
    "df_daily = df_all_cols[daily_rows]\n",
    "df_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:15.308473Z",
     "start_time": "2022-04-19T19:07:14.329063Z"
    }
   },
   "outputs": [],
   "source": [
    "## Converting data to \"float\"\n",
    "for col in df_daily.columns:\n",
    "    df_daily.loc[:,col] = pd.to_numeric(df_daily.loc[:,col], downcast = 'float')\n",
    "    \n",
    "df_daily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Summary for Posting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**My main goal is to create a simple, easy-to-use report that I can roll-out quickly and easily.** Ideally, the summary will be intuitive, clear, and allow for the night auditor to post the charges quickly and easily.\n",
    "\n",
    "I know that there are only a select few columns that we reference for posting charges, so I will start my summary by creating a small dataframe summarizing the usual charges.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:15.324402Z",
     "start_time": "2022-04-19T19:07:15.309481Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Summarize food charges by outlet\n",
    "daily_food = df_daily['Food'] + df_daily['Other'] - df_daily['Discount']\n",
    "daily_food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:15.340390Z",
     "start_time": "2022-04-19T19:07:15.328392Z"
    }
   },
   "outputs": [],
   "source": [
    "## Summarize tax charges by outlet\n",
    "daily_tax = df_daily['MD Food 6%']+df_daily['MD Liq 9%']\n",
    "daily_tax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:15.356341Z",
     "start_time": "2022-04-19T19:07:15.342354Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Summarize all charges by outlet\n",
    "summary = pd.concat([daily_food, df_daily['Beverage'], daily_tax, \n",
    "           df_daily['Tip collected']], axis=1)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:15.372304Z",
     "start_time": "2022-04-19T19:07:15.357313Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating new labels\n",
    "\n",
    "col_labels = {0: \"Food\", 1: \"Tax\", 'Tip collected': 'Gratuity'}\n",
    "\n",
    "row_labels = {'Bar': \"Lobby Bar\", \"Dining Room\": 'Rain 903', \n",
    "              'Room Service': 'In-Room Dining', 'Starbucks': 'Coffee Corner'}\n",
    "\n",
    "col_labels, row_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:15.388231Z",
     "start_time": "2022-04-19T19:07:15.373300Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Renaming columns\n",
    "summary = summary.rename(columns = col_labels, index = row_labels)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:15.404217Z",
     "start_time": "2022-04-19T19:07:15.389228Z"
    }
   },
   "outputs": [],
   "source": [
    "## Rounding values - preventing long numbers\n",
    "summary = summary.applymap(lambda x: round(x, 2))\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:15.420174Z",
     "start_time": "2022-04-19T19:07:15.406183Z"
    }
   },
   "outputs": [],
   "source": [
    "# summary.to_excel('Test_Summary_Workbook.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing the Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "First summary working - need to expand\n",
    "\n",
    "* Repeat process per MOP\n",
    "    * A&G\n",
    "    * RC\n",
    "    * CA\n",
    "    * AX\n",
    "    * DI\n",
    "    * VI/MC\n",
    "    \n",
    "* Save results to different sheets?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ∆í: Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    " - [ ] Create workflow function to reuse w/ each report\n",
    " - [ ] Test specific MOP reports\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:15.436133Z",
     "start_time": "2022-04-19T19:07:15.421175Z"
    }
   },
   "outputs": [],
   "source": [
    "def cleaning_mops(file_path, new_file = None):\n",
    "\n",
    "    df = pd.read_excel(file_path, skiprows = 2)\n",
    "\n",
    "    df.columns = ['Label', *df.columns[:-1]]\n",
    "\n",
    "    col_labels = ['Label', 'Bar', 'Dining Room', 'Room Service', 'Starbucks']\n",
    "    drop_cols = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col not in col_labels:\n",
    "            drop_cols.append(col)\n",
    "\n",
    "    df = df.drop(columns = drop_cols)\n",
    "\n",
    "    df_cleaned = df[df.isna().sum(axis=1) == 0]\n",
    "    df_cleaned = df_cleaned[~df_cleaned.loc[:, \"Label\"].isin([' '])]\n",
    "    df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "\n",
    "    df_all_cols = df_cleaned.T\n",
    "\n",
    "    df_all_cols.columns = df_all_cols.iloc[0]\n",
    "    df_all_cols = df_all_cols.drop(index = 'Label')\n",
    "\n",
    "    daily_rows = ['Food', 'Beverage', 'Other', 'Discount', 'MD Food 6%',\n",
    "              'MD Liq 9%', 'Tip collected', \"ALL A&G CHRG\", 'Room Charge',\n",
    "              'American Express', 'Cash', 'Discover Card', 'MasterCard', 'Visa']\n",
    "\n",
    "    df_daily = df_all_cols.loc[:,daily_rows]\n",
    "\n",
    "    for col in df_daily.columns:\n",
    "        df_daily.loc[:,col] = pd.to_numeric(df_daily.loc[:,col], downcast = 'float')\n",
    "\n",
    "    daily_food = df_daily.loc[:,'Food'] + df_daily.loc[:,'Other'] - df_daily.loc[:,'Discount']\n",
    "\n",
    "    daily_tax = df_daily.loc[:,'MD Food 6%'] + df_daily.loc[:,'MD Liq 9%']\n",
    "\n",
    "    summary = pd.concat([daily_food, df_daily.loc[:,'Beverage'], daily_tax, \n",
    "           df_daily.loc[:,'Tip collected']], axis=1)\n",
    "    \n",
    "    col_labels = {0: \"Food\", 1: \"Tax\", 'Tip collected': 'Gratuity'}\n",
    "\n",
    "    row_labels = {'Bar': \"Lobby Bar\", \"Dining Room\": 'Rain 903', \n",
    "              'Room Service': 'In-Room Dining', 'Starbucks': 'Coffee Corner'}\n",
    "\n",
    "    summary = summary.rename(columns = col_labels, index = row_labels)\n",
    "\n",
    "    summary = summary.applymap(lambda x: round(x, 2))\n",
    "\n",
    "#     summary.to_excel(new_file)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:15.468048Z",
     "start_time": "2022-04-19T19:07:15.438098Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df = cleaning_mops(file_path = './data/02_25_2022_mop_det_VIMC.xls')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:15.484005Z",
     "start_time": "2022-04-19T19:07:15.469014Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_df['MoP'] ='VI/MC'\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:15.499958Z",
     "start_time": "2022-04-19T19:07:15.485002Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df = test_df.reset_index()\n",
    "test_df = test_df.set_index(['MoP', 'index'])\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:15.531847Z",
     "start_time": "2022-04-19T19:07:15.500968Z"
    }
   },
   "outputs": [],
   "source": [
    "mop = 'AG'\n",
    "file_path = f'./data/02_25_2022_mop_det_{mop}.xls'\n",
    "results = cleaning_mops(file_path)\n",
    "results.loc[:,'MoP'] = mop.upper()\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:15.658535Z",
     "start_time": "2022-04-19T19:07:15.533841Z"
    }
   },
   "outputs": [],
   "source": [
    "list_mops = ['ag', 'rc', 'ax', 'ca', 'di', 'vimc']\n",
    "\n",
    "results = pd.DataFrame()\n",
    "\n",
    "for mop in list_mops:\n",
    "    file_path = f'./data/02_25_2022_mop_det_{mop}.xls'\n",
    "    results = cleaning_mops(file_path)\n",
    "    results.loc[:,'MoP'] = mop.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:15.674496Z",
     "start_time": "2022-04-19T19:07:15.659516Z"
    }
   },
   "outputs": [],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:15.690455Z",
     "start_time": "2022-04-19T19:07:15.677458Z"
    }
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So Far..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**‚àö: MVP**\n",
    "\n",
    "- [x] Created function to clean, organize data from summary reports\n",
    " - [x] Tested results by looping through list of files\n",
    "\n",
    "\n",
    "**TD: MVP**\n",
    "\n",
    " - [ ] Add additional column identifying MOP\n",
    " - [ ] Create multi-index w/ MOP, PC\n",
    " - [ ] Concat DFs\n",
    " - [ ] Save results to XL\n",
    " - [ ] Convert to script\n",
    " \n",
    " \n",
    "**TD: AAB**\n",
    "\n",
    " - [ ] Create filters to include extra details\n",
    "     * *Ex: Discounts, charges, any new MOPs, payments other than listed MOPs*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.15.22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: MVP up & running\n",
    "\n",
    " - [ ] Check status\n",
    " - [ ] Add/create multi-index for MOP\n",
    " - [ ] Create SINGLE results XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:15.706382Z",
     "start_time": "2022-04-19T19:07:15.691450Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def cleaning_mops_test(file_path, mop_name = None, new_file_name = None):\n",
    "    \n",
    "    ## Read file\n",
    "    df = pd.read_excel(file_path, skiprows = 2)\n",
    "    \n",
    "    ## Shift label names one to the right and drop extra column\n",
    "    df.columns = ['Label', *df.columns[:-1]]\n",
    "    \n",
    "    ## Specify features to keep\n",
    "    col_labels = ['Label', 'Bar', 'Dining Room', 'Room Service', 'Starbucks']\n",
    "    drop_cols = []\n",
    "    \n",
    "    ## Create list of column names to drop\n",
    "    for col in df.columns:\n",
    "        if col not in col_labels:\n",
    "            drop_cols.append(col)\n",
    "    \n",
    "    ## Drop columns\n",
    "    df = df.drop(columns = drop_cols)\n",
    "    \n",
    "    ## Drop rows with NaNs and/or blank labels\n",
    "    df_cleaned = df[df.isna().sum(axis=1) == 0]\n",
    "    df_cleaned = df_cleaned[~df_cleaned.loc[:, \"Label\"].isin([' '])]\n",
    "    df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "    \n",
    "    ## Transpose for easy reading\n",
    "    df_all_cols = df_cleaned.T\n",
    "    \n",
    "    ## Relabel based on values in first row\n",
    "    df_all_cols.columns = df_all_cols.iloc[0]\n",
    "    df_all_cols = df_all_cols.drop(index = 'Label')\n",
    "\n",
    "    daily_rows = ['Food', 'Beverage', 'Other', 'Discount', 'MD Food 6%',\n",
    "              'MD Liq 9%', 'Tip collected', \"ALL A&G CHRG\", 'Room Charge',\n",
    "              'American Express', 'Cash', 'Discover Card', 'MasterCard', 'Visa']\n",
    "    \n",
    "    ## Slicing out specific labels of interest\n",
    "    df_daily = df_all_cols.loc[:, daily_rows]\n",
    "    \n",
    "    ## Recast datatypes\n",
    "    for col in df_daily.columns:\n",
    "        df_daily.loc[:,col] = pd.to_numeric(df_daily.loc[:,col], downcast = 'float')\n",
    "\n",
    "    ## Generate totals for each outlet\n",
    "    daily_food = df_daily['Food'] + df_daily['Other'] - df_daily['Discount']\n",
    "\n",
    "    daily_tax = df_daily['MD Food 6%'] + df_daily['MD Liq 9%']\n",
    "\n",
    "    summary = pd.concat([daily_food, df_daily.loc[:,'Beverage'], daily_tax,\n",
    "                         df_daily.loc[:,'Tip collected']], axis=1)\n",
    "    \n",
    "    ## Relabel columns/rows\n",
    "    col_labels = {0: \"Food\", 1: \"Tax\", 'Tip collected': 'Gratuity'}\n",
    "    row_labels = {'Bar': \"Lobby Bar\", \"Dining Room\": 'Rain 903', \n",
    "              'Room Service': 'In-Room Dining', 'Starbucks': 'Coffee Corner'}\n",
    "        \n",
    "    summary = summary.rename(columns = col_labels, index = row_labels)\n",
    "    \n",
    "    ## Round to prevent unnecessary decimals\n",
    "    summary = summary.applymap(lambda x: round(x, 2))\n",
    "    \n",
    "    ## Generate new index to group MOPs\n",
    "    if mop_name != None:\n",
    "        summary.loc[:,'Payment'] = mop_name.upper()\n",
    "        summary = summary.reset_index()\n",
    "        summary = summary.rename(columns = {'index':'Outlet'})\n",
    "        summary = summary.set_index(['Payment', 'Outlet'])\n",
    "        \n",
    "#     summary.to_excel(new_file)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:15.753255Z",
     "start_time": "2022-04-19T19:07:15.708404Z"
    }
   },
   "outputs": [],
   "source": [
    "## For a specified MOP, pull specific file and run cleaning function\n",
    "\n",
    "mop = 'vimc'\n",
    "\n",
    "file_path = f'./data/02_25_2022_mop_det_{mop}.xls'\n",
    "\n",
    "cleaning_mops_test(file_path=file_path, mop_name=mop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:15.928814Z",
     "start_time": "2022-04-19T19:07:15.754252Z"
    }
   },
   "outputs": [],
   "source": [
    "## Run all MOPs through function\n",
    "\n",
    "list_mops = ['ag', 'rc', 'ax', 'ca', 'di', 'vimc']\n",
    "\n",
    "new_results = []\n",
    "\n",
    "for mop in list_mops:\n",
    "    file_path = f'./data/02_25_2022_mop_det_{mop}.xls'\n",
    "    new_results.append(cleaning_mops_test(file_path=file_path, mop_name=mop))\n",
    "    \n",
    "new_results_df = pd.concat(new_results)\n",
    "new_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:15.944772Z",
     "start_time": "2022-04-19T19:07:15.929813Z"
    }
   },
   "outputs": [],
   "source": [
    "# new_results_df.to_excel('./data/02_25_2022_summary.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Alerts for RC: Dupes and A&G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Two possible errors for Room Charge settlements:\n",
    "    * Duplicate checks\n",
    "    * Charged to A&G\n",
    "* Need report to check guest names to ID and warn of these postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:16.134237Z",
     "start_time": "2022-04-19T19:07:15.946767Z"
    }
   },
   "outputs": [],
   "source": [
    "ra_rc_df = pd.read_excel(io = './data/april_2022_rc_receipts.xls', header = None, skiprows = 1,\n",
    "                        names = ['Table #', 'Check #', 'Server #', 'Server Name', 'Cashier #',\n",
    "                                 'Cashier Name','MoP', 'Profit Center', 'Blank1', 'Blank2',\n",
    "                                 'Tip', 'Total Receipt', 'Guest Name/Room', 'Date', 'Time'])\n",
    "ra_rc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:16.290817Z",
     "start_time": "2022-04-19T19:07:16.136272Z"
    }
   },
   "outputs": [],
   "source": [
    "## Dropping total rows at end of report\n",
    "ra_rc_df = ra_rc_df[:-3]\n",
    "ra_rc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:16.306774Z",
     "start_time": "2022-04-19T19:07:16.291818Z"
    }
   },
   "outputs": [],
   "source": [
    "bool_idx = ra_rc_df.loc[:,'Guest Name/Room'].str.startswith(('A&G', 'Dup'))\n",
    "bool_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:16.322749Z",
     "start_time": "2022-04-19T19:07:16.307802Z"
    }
   },
   "outputs": [],
   "source": [
    "## Slicing out checks that meet the criteria\n",
    "ra_rc_df[bool_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:16.338721Z",
     "start_time": "2022-04-19T19:07:16.324727Z"
    }
   },
   "outputs": [],
   "source": [
    "chk_num = list(ra_rc_df[bool_idx]['Check #'].values)\n",
    "chk_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Python Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "* [Real Python](https://realpython.com/python-logging/)\n",
    "* [Python Docs](https://docs.python.org/3/howto/logging.html#logging-basic-tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:16.354649Z",
     "start_time": "2022-04-19T19:07:16.339717Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Review check numbers {chk_num} for duplicate check/direct A&G postings!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:16.370634Z",
     "start_time": "2022-04-19T19:07:16.356647Z"
    }
   },
   "outputs": [],
   "source": [
    "if len(chk_num) >0:\n",
    "    logger.warning(f'Review checks for possible errors: {chk_num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint - 4.19.22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Current Status:**\n",
    "\n",
    "* Read Digital Dining report data *only for Excel files*\n",
    "    * AAB: read DBF files directly\n",
    "    * AAB: review sales type for discounts, subtract from proper sales type\n",
    "        * Currently subtracted from food generally\n",
    "* Clean the data to generate summaries about food/bev/tax/gratuity amounts\n",
    "    * MVP: confirm process works for all MOPs\n",
    "* Log warnings about any \"Duplicate Check\" or direct A&G postings\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "1. Test on most recent files\n",
    "2. Artificially generate errors (then cancel before NA)\n",
    "3. Condense into script to run on server\n",
    "4. Test script\n",
    "5. Schedule script to run daily\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two parts:\n",
    "\n",
    "1. Run through workflow above using the overall summary data\n",
    "2. Loop through a list of the usual MOPs and create a multi-indexed DataFrame containing the results for each outlet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:16.925154Z",
     "start_time": "2022-04-19T19:07:16.371632Z"
    }
   },
   "outputs": [],
   "source": [
    "workflow(data_filepath ='./data/02_25_2022_mop_det_vimc.xls', summary_file_name = 'test_workflow_results.xlsx',\n",
    "         receipts_filepath ='./data/april_2022_rc_receipts.xls', log_filepath = 'test_workflow_log.log',\n",
    "        mop_name = 'vimc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:16.940112Z",
     "start_time": "2022-04-19T19:07:16.927117Z"
    }
   },
   "outputs": [],
   "source": [
    "def new_loop(summary_file_name, receipts_filepath, log_filepath):\n",
    "    \n",
    "    list_mops = ['ag', 'rc', 'ax', 'ca', 'di', 'vimc']\n",
    "\n",
    "    new_results = []\n",
    "    for mop in list_mops:\n",
    "        file_path = f'./data/02_25_2022_mop_det_{mop}.xls'\n",
    "        summary_file_name = f'results_for_{mop}.xlsx'\n",
    "        new_results.append(workflow(data_filepath = file_path, mop_name = mop,\n",
    "                                    summary_file_name = summary_file_name,\n",
    "                                    receipts_filepath = receipts_filepath,\n",
    "                                    log_filepath = log_filepath))\n",
    "\n",
    "#     new_results_df = pd.concat(new_results, axis = 0)\n",
    "\n",
    "    return new_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.339047Z",
     "start_time": "2022-04-19T19:07:16.943073Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_loop(summary_file_name = 'test_workflow_results.xlsx',\n",
    "              receipts_filepath ='./data/april_2022_rc_receipts.xls',\n",
    "              log_filepath = 'test_workflow_log.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.355002Z",
     "start_time": "2022-04-19T19:07:17.341020Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def workflow(data_filepath, mop_name = None):#, summary_file_name, receipts_filepath, log_filepath):\n",
    "#     \"\"\"Processes entire workflow to generate summary data for Digital Dining reports.\n",
    "\n",
    "#     Reads files; cleans; summarizes by MOP; generates log files to confirm any issues.\n",
    "\n",
    "#     Args:\n",
    "#         data_filepath (str): Path to data file (.xls/.xlsx files only)\n",
    "#         summary_file_name (str): Name for results file\n",
    "#         receipts_filepath (str): Receipts Audit report file\n",
    "#         log_filepath (str): Name for log file.\n",
    "#     \"\"\"\n",
    "\n",
    "# #     logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s', \n",
    "# #                         datefmt='%d-%b-%y %H:%M:%S',filename=log_filepath,\n",
    "# #                         encoding='utf-8',  filemode='a')\n",
    "\n",
    "# #     # Create logger and set level\n",
    "# #     logger = logging.getLogger('Report_Log')\n",
    "# #     logger.setLevel(logging.INFO)\n",
    "\n",
    "# #     # Create log file handler\n",
    "# #     fh = logging.FileHandler(log_filepath)\n",
    "# #     fh.setLevel(logging.INFO)\n",
    "\n",
    "# #     formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "# #     fh.setFormatter(formatter)\n",
    "\n",
    "# #     logger.addHandler(fh)\n",
    "\n",
    "\n",
    "\n",
    "#     ## Read all columns from file (incl. empty columns)\n",
    "#     df = pd.read_excel(data_filepath,  skiprows = 2)\n",
    "\n",
    "#     ## Shifting label names one to the right and drop extra column\n",
    "#     df.columns = ['Label', *df.columns[:-1]]\n",
    "\n",
    "#     ## Creating list of columns to keep/drop\n",
    "#     col_labels = ['Label', 'Bar', 'Dining Room', 'Room Service', 'Starbucks']\n",
    "#     drop_cols = []\n",
    "\n",
    "#     for col in df.columns:\n",
    "#         if col not in col_labels:\n",
    "#             drop_cols.append(col)\n",
    "\n",
    "#     ## Dropping columns not representing an outlet\n",
    "#     df = df.drop(columns = drop_cols)\n",
    "\n",
    "#     df_cleaned = df[df.isna().sum(axis=1) == 0]\n",
    "#     df_cleaned = df_cleaned[~df_cleaned.loc[:, \"Label\"].isin([' '])]\n",
    "#     df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "\n",
    "#     df_all_cols = df_cleaned.copy().T\n",
    "#     df_all_cols\n",
    "\n",
    "#     ## Re-labeling columns based on \"Label\" row\n",
    "#     df_all_cols.columns = df_all_cols.iloc[0]\n",
    "#     df_all_cols = df_all_cols.drop(index = 'Label')\n",
    "\n",
    "#     daily_rows = ['Food', 'Beverage', 'Other', 'Discount', 'MD Food 6%',\n",
    "#                 'MD Liq 9%', 'Tip collected', \"ALL A&G CHRG\", 'Room Charge',\n",
    "#                 'American Express', 'Cash', 'Discover Card', 'MasterCard', 'Visa']\n",
    "\n",
    "#     df_daily = df_all_cols[daily_rows]\n",
    "\n",
    "#     ## Converting data to \"float\"\n",
    "# #     for col in df_daily.columns:\n",
    "# #         df_daily.loc[:,col] = pd.to_numeric(df_daily.loc[:,col], downcast = 'float')\n",
    "#     df_daily = df_daily.applymap(lambda x: float(x))\n",
    "\n",
    "#     ## Summarize food charges by outlet\n",
    "#     daily_food = df_daily['Food'] + df_daily['Other'] - df_daily['Discount']\n",
    "\n",
    "#     ## Summarize tax charges by outlet\n",
    "#     daily_tax = df_daily['MD Food 6%']+df_daily['MD Liq 9%']\n",
    "\n",
    "#     ## Summarize all charges by outlet\n",
    "#     summary = pd.concat([daily_food, df_daily['Beverage'], daily_tax, \n",
    "#             df_daily['Tip collected']], axis=1)\n",
    "\n",
    "#     ## Creating new labels\n",
    "#     col_labels = {0: \"Food\", 1: \"Tax\", 'Tip collected': 'Gratuity'}\n",
    "#     row_labels = {'Bar': \"Lobby Bar\", \"Dining Room\": 'Rain 903', \n",
    "#                 'Room Service': 'In-Room Dining', 'Starbucks': 'Coffee Corner'}\n",
    "\n",
    "#     ## Renaming columns\n",
    "#     summary = summary.rename(columns = col_labels, index = row_labels)\n",
    "\n",
    "#     ## Rounding values - preventing long numbers\n",
    "#     summary = summary.applymap(lambda x: round(x, 2))\n",
    "    \n",
    "#     ## Generate new index to group MOPs\n",
    "#     if mop_name != None:\n",
    "#         summary.loc[:,'Payment'] = mop_name.upper()\n",
    "#         summary = summary.reset_index()\n",
    "#         summary = summary.rename(columns = {'index':'Outlet'})\n",
    "#         summary = summary.set_index(['Payment', 'Outlet'])\n",
    "\n",
    "# #     # summary.to_excel(summary_file_name, engine = 'openpyxl')\n",
    "\n",
    "# #     ra_rc_df = pd.read_excel(io = receipts_filepath, header = None, skiprows = 1,\n",
    "# #                         names = ['Table #', 'Check #', 'Server #', 'Server Name', 'Cashier #',\n",
    "# #                                 'Cashier Name','MoP', 'Profit Center', 'Blank1', 'Blank2',\n",
    "# #                                 'Tip', 'Total Receipt', 'Guest Name/Room', 'Date', 'Time'])\n",
    "    \n",
    "# #     ## Dropping total rows at end of report\n",
    "# #     ra_rc_df = ra_rc_df[:-3]\n",
    "\n",
    "# #     bool_idx = ra_rc_df.loc[:,'Guest Name/Room'].str.startswith(('A&G', 'Dup'))\n",
    "\n",
    "# #     chk_num = list(ra_rc_df[bool_idx]['Check #'].values)\n",
    "\n",
    "# #     if len(chk_num) >0:\n",
    "# #         logger.warning(f'Review checks for possible errors: {chk_num}')\n",
    "# #     else:\n",
    "# #         logger.info(f'There are no checks to review.')\n",
    "\n",
    "#     return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.370961Z",
     "start_time": "2022-04-19T19:07:17.356970Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# workflow(data_filepath = './data/02_25_2022_pc.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.386876Z",
     "start_time": "2022-04-19T19:07:17.371927Z"
    }
   },
   "outputs": [],
   "source": [
    "# def loop_workflow():#, summary_file_name, receipts_filepath, log_filepath):\n",
    "\n",
    "    \n",
    "#     list_mops = ['ag', 'rc', 'ax', 'ca', 'di', 'vimc']\n",
    "#     new_results = []\n",
    "#     for mop in list_mops:\n",
    "#         data_filepath = f'./data/02_25_2022_mop_det_{mop}.xls'\n",
    "#         summary_file_name = f'results_for_{mop}.xlsx'\n",
    "#         new_results.append(workflow(data_filepath, mop_name = mop))#, summary_file_name, receipts_filepath, log_filepath))\n",
    "        \n",
    "#     new_results_df = pd.concat(new_results)\n",
    "\n",
    "#     return new_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.402830Z",
     "start_time": "2022-04-19T19:07:17.388871Z"
    }
   },
   "outputs": [],
   "source": [
    "# from functions.script import *\n",
    "# from functions.loop_script import loop_workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.417789Z",
     "start_time": "2022-04-19T19:07:17.403830Z"
    }
   },
   "outputs": [],
   "source": [
    "# def loop_workflow(data_filepath, summary_file_name, receipts_filepath, log_filepath):\n",
    "\n",
    "    \n",
    "#     list_mops = ['ag', 'rc', 'ax', 'ca', 'di', 'vimc']\n",
    "#     new_results = []\n",
    "#     for mop in list_mops:\n",
    "#         file_path = f'./data/02_25_2022_mop_det_{mop}.xls'\n",
    "#         summary_file_name = f'results_for_{mop}.xlsx'\n",
    "#         new_results.append(workflow(data_filepath, summary_file_name, receipts_filepath, log_filepath))\n",
    "        \n",
    "#     new_results_df = pd.concat(new_results)\n",
    "\n",
    "#     return new_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.432750Z",
     "start_time": "2022-04-19T19:07:17.418758Z"
    }
   },
   "outputs": [],
   "source": [
    "# loop_workflow(summary_file_name = 'test_workflow_results.xlsx',\n",
    "#               receipts_filepath ='./data/april_2022_rc_receipts.xls',\n",
    "#               log_filepath = 'test_workflow_log.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.448707Z",
     "start_time": "2022-04-19T19:07:17.433747Z"
    }
   },
   "outputs": [],
   "source": [
    "# list_mops = ['ag', 'rc', 'ax', 'ca', 'di', 'vimc']\n",
    "\n",
    "# new_results = []\n",
    "# for mop in list_mops:\n",
    "#     file_path = f'./data/02_25_2022_mop_det_{mop}.xls'\n",
    "#     summary_file_name = f'results_for_{mop}.xlsx'\n",
    "#     new_results.append(workflow(data_filepath = file_path, mop_name = mop,\n",
    "#                                 summary_file_name = 'test_workflow_results.xlsx',\n",
    "#                                 receipts_filepath ='./data/april_2022_rc_receipts.xls',\n",
    "#                                 log_filepath = 'test_workflow_log.log'))\n",
    "\n",
    "# new_results_df = pd.concat(new_results, axis = 0)\n",
    "\n",
    "# new_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.464687Z",
     "start_time": "2022-04-19T19:07:17.449704Z"
    }
   },
   "outputs": [],
   "source": [
    "# loop_workflow(summary_file_name = 'test_workflow_results.xlsx',\n",
    "#               receipts_filepath ='./data/april_2022_rc_receipts.xls',\n",
    "#               log_filepath = 'test_workflow_log.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.480621Z",
     "start_time": "2022-04-19T19:07:17.467628Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# receipts_filepath ='./data/april_2022_rc_receipts.xls'\n",
    "# log_filepath = 'test_workflow_log.log'\n",
    "\n",
    "# list_mops = ['ag', 'rc', 'ax', 'ca', 'di', 'vimc']\n",
    "\n",
    "# new_results = []\n",
    "\n",
    "# for mop in list_mops:\n",
    "#     file_path = f'./data/02_25_2022_mop_det_{mop}.xls'\n",
    "#     summary_file_name = f'results_for_{mop}.xlsx'\n",
    "#     new_results.append(workflow(data_filepath = file_path, summary_file_name = summary_file_name,\n",
    "#                                 receipts_filepath = receipts_filepath, log_filepath = log_filepath))\n",
    "\n",
    "# new_results_df = pd.concat(new_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö†Ô∏è Old Code - Raises Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.544452Z",
     "start_time": "2022-04-19T19:07:17.482628Z"
    }
   },
   "outputs": [],
   "source": [
    "raise Exception (\"End of Development - Resume here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.546416Z",
     "start_time": "2022-04-19T19:07:11.174Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Review datatypes\n",
    "\n",
    "df_no_drp.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.547413Z",
     "start_time": "2022-04-19T19:07:11.177Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Recast columns to \"float\" datatype & review\n",
    "\n",
    "for col in df_no_drp.columns:\n",
    "    df_no_drp[col] = pd.to_numeric(df_no_drp[col], downcast = 'float')\n",
    "\n",
    "print(df_no_drp.dtypes)\n",
    "\n",
    "display(df_no_drp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.549418Z",
     "start_time": "2022-04-19T19:07:11.180Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Inspect results\n",
    "\n",
    "df_no_drp[df_no_drp.columns[:-1]].T.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.550405Z",
     "start_time": "2022-04-19T19:07:11.183Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## Create new total column\n",
    "\n",
    "# df_no_drp['Total'] = df_no_drp.sum(axis=1)\n",
    "# df_no_drp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.550405Z",
     "start_time": "2022-04-19T19:07:11.186Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Filter rows without any activity\n",
    "\n",
    "## CURRENTLY UNUSED - FILTERING BELOW ##\n",
    "\n",
    "# df_no_zero = df_no_drp[df_no_drp['Total'] > 0]\n",
    "# df_no_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.551402Z",
     "start_time": "2022-04-19T19:07:11.189Z"
    }
   },
   "outputs": [],
   "source": [
    "## Identify all unique labels from total data\n",
    "\n",
    "df_no_drp.index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.552403Z",
     "start_time": "2022-04-19T19:07:11.191Z"
    }
   },
   "outputs": [],
   "source": [
    "## Create list of frequently-used labels\n",
    "\n",
    "freq_cats = ['Food', 'Beverage', 'Other','Discount', 'Charge','MD Food 6%',\n",
    "             'MD Liq 9%','Tip collected','ALL A&G CHRG', 'Room Charge',\n",
    "             'American Express', 'Cash', 'Discover Card', 'MasterCard',\n",
    "             'Visa']\n",
    "freq_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.553397Z",
     "start_time": "2022-04-19T19:07:11.194Z"
    }
   },
   "outputs": [],
   "source": [
    "## Filtering frequent labels from total data\n",
    "\n",
    "df_no_drp.loc[freq_cats, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.553397Z",
     "start_time": "2022-04-19T19:07:11.197Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating lists to subset the data\n",
    "\n",
    "cats_profit_centers = df_no_drp.columns[:-1]\n",
    "cats_revenue = ['Food','Beverage','Other','Charge']\n",
    "cats_disc = ['Discount']\n",
    "cats_tax = ['MD Food 6%', 'MD Liq 9%']\n",
    "cats_tips = ['Tip collected']\n",
    "cats_mops = ['ALL A&G CHRG', 'American Express', 'Room Charge', 'Cash',\n",
    "             'Discover Card', 'MasterCard', 'Visa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.554394Z",
     "start_time": "2022-04-19T19:07:11.200Z"
    }
   },
   "outputs": [],
   "source": [
    "# bar_post_rev = {}\n",
    "\n",
    "# for cat in cats_revenue:\n",
    "# #     bar_post_rev[cat] = df_no_drp.loc[freq_cats,cat]\n",
    "#     print(df_no_drp.loc[cat, 'Lounge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   ‚úîÔ∏è FIXED ERROR ‚úîÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Need to switch to individual MOP reports\n",
    "    * PC report shows total sums\n",
    "    * Need to break down by MOP for proper details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.555392Z",
     "start_time": "2022-04-19T19:07:11.204Z"
    }
   },
   "outputs": [],
   "source": [
    "## Pulling yesterday's files\n",
    "## Useful for processing during Daily reporting, but not same-day\n",
    "# yesterday = datetime.now() - timedelta(1)    \n",
    "\n",
    "# yd_str = datetime.strftime(yesterday, '%m_%d_%Y')\n",
    "# yd_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.556389Z",
     "start_time": "2022-04-19T19:07:11.207Z"
    }
   },
   "outputs": [],
   "source": [
    "## Pulling today's files\n",
    "\n",
    "today = datetime.now()   \n",
    "\n",
    "td_str = datetime.strftime(today, '%m_%d_%Y')\n",
    "td_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.556389Z",
     "start_time": "2022-04-19T19:07:11.210Z"
    }
   },
   "outputs": [],
   "source": [
    "list_mops = ['ag', 'ax', 'ca', 'di', 'rc', 'vimc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.557386Z",
     "start_time": "2022-04-19T19:07:11.212Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_data(file_path):\n",
    "    \"\"\"Work flow to import, clean, and eventually save the results as a new report.\n",
    "\n",
    "    Args:\n",
    "        file_path (string): file path for the original profit center report\n",
    "    \"\"\"    \n",
    "\n",
    "    ## Read specific columns from Excel file\n",
    "\n",
    "    df = pd.read_excel(file_path, skiprows = 2)\n",
    "\n",
    "    ## Reassign label names: shifting to the right and dropping extra\n",
    "\n",
    "    df.columns = ['Label', *df.columns[:-1]]\n",
    "    ## Creating list of columns to keep/drop\n",
    "\n",
    "    col_labels = ['Label', 'Bar', 'Dining Room', 'Room Service', 'Starbucks',\n",
    "                'Page Total']\n",
    "\n",
    "    drop_cols = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col not in col_labels:\n",
    "            drop_cols.append(col)\n",
    "            \n",
    "    ## Dropping columns not representing an outlet\n",
    "\n",
    "    df = df.drop(columns = drop_cols)\n",
    "\n",
    "    ## Filter out those rows with missing/blank values\n",
    "\n",
    "    df_cleaned = df[df.isna().sum(axis=1) == 0]\n",
    "    df_cleaned = df_cleaned[~df_cleaned.loc[:, \"Label\"].isin([' '])]\n",
    "    df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "\n",
    "    ## Create new DF to allow for later reuse\n",
    "\n",
    "    df_no_drp = df_cleaned.copy()\n",
    "\n",
    "    ## Relabel and reset index with correct labels\n",
    "\n",
    "    df_no_drp = df_no_drp.rename(df_no_drp[\"Label\"])\n",
    "    df_no_drp = df_no_drp.drop(columns = \"Label\")\n",
    "\n",
    "    ## Recast columns to \"float\" datatype & review\n",
    "\n",
    "    for col in df_no_drp.columns:\n",
    "        df_no_drp[col] = pd.to_numeric(df_no_drp[col], downcast = 'float')\n",
    "    \n",
    "#     ## Create list of frequently-used labels\n",
    "\n",
    "#     freq_cats = ['Food', 'Beverage', 'Other','Discount', 'Charge','MD Food 6%',\n",
    "#              'MD Liq 9%','Tip collected','ALL A&G CHRG', 'Room Charge',\n",
    "#              'American Express', 'Cash', 'Discover Card', 'MasterCard',\n",
    "#              'Visa']\n",
    "\n",
    "    return df_no_drp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.558384Z",
     "start_time": "2022-04-19T19:07:11.215Z"
    }
   },
   "outputs": [],
   "source": [
    "## Pulling all MOP reports and saving to dict\n",
    "\n",
    "if use_ref_rpt == True:\n",
    "    yd_str = '02_25_2022'\n",
    "else:\n",
    "    yd_str = datetime.strftime(yesterday, '%m_%d_%Y')\n",
    "\n",
    "dict_reports = {}\n",
    "\n",
    "for mop in list_mops:\n",
    "    dict_reports[mop] = clean_data(f'./data/{yd_str}_mop_det_{mop}.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.559390Z",
     "start_time": "2022-04-19T19:07:11.218Z"
    }
   },
   "outputs": [],
   "source": [
    "## Review resulting keys\n",
    "dict_reports.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.560385Z",
     "start_time": "2022-04-19T19:07:11.221Z"
    }
   },
   "outputs": [],
   "source": [
    "## Check number of columns in each report\n",
    "for key in dict_reports:\n",
    "    print(key, dict_reports[key].shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è ‚öíÔ∏è TODO: ‚öíÔ∏è ‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Filter non-zero dataframe into common/uncommon categories\n",
    "2. Create DF w/ LS XAC for normal use\n",
    "3. Create DF w/ unusual cats w/ prompt to review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow 1: Dropping Unused Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Dropping Labels**\n",
    "\n",
    "> Currently, I ignore several sections of the report as they do not report any relevant details. However, **this may lead us to overlook any irregular data.**\n",
    ">\n",
    "> This work flow focused on the most commonly used section of the report, highlighting the details used for NA.\n",
    ">\n",
    "> While I prefer a more inclusive approach, I am keeping this work flow for reference.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.561376Z",
     "start_time": "2022-04-19T19:07:11.228Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Identify unique labels to determine which to drop\n",
    "\n",
    "df_cleaned.loc[:, \"Label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.562373Z",
     "start_time": "2022-04-19T19:07:11.231Z"
    }
   },
   "outputs": [],
   "source": [
    "## Create list of labels to drop\n",
    "\n",
    "drop_labels = ['Sales Subtotal', 'Total before tax', 'None', 'None', \n",
    "               'Tax Subtotal','Receipt Rounding','Total of check', ' ', 'Unadjusted receipts',\n",
    "               'PaidIn - AR', 'PaidIn - Debitek','PaidIn - Gift Certificates',\n",
    "               'No of Covers', 'No of Checks', 'Avg cover', 'Avg check',\n",
    "               'Total Paid Outs', 'Server bank', 'Total tips',\n",
    "               'Server credit card fees', 'Net tips', 'Net Cash',\n",
    "               'Other receipts turned in', 'Server Drops', 'Turned in',\n",
    "               'Restaurant credit card fees', 'Estimated deposit',\n",
    "               'Beginning NRS Total', 'Total Receipts', 'Discounts',\n",
    "               'Paid Ins','Gross Total', 'NRS Total']\n",
    "drop_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.563370Z",
     "start_time": "2022-04-19T19:07:11.234Z"
    }
   },
   "outputs": [],
   "source": [
    "## Drop specified labels and save to new DF\n",
    "\n",
    "df_new_idx =df_cleaned[~df_cleaned.loc[:, \"Label\"].isin(drop_labels)]\n",
    "df_new_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.565365Z",
     "start_time": "2022-04-19T19:07:11.236Z"
    }
   },
   "outputs": [],
   "source": [
    "## Rename index with remaining labels\n",
    "\n",
    "df_new_idx = df_new_idx.rename(df_cleaned[\"Label\"])\n",
    "df_new_idx = df_new_idx.drop(columns = \"Label\")\n",
    "df_new_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è CONDENSE W/ UPPER WF ‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.566392Z",
     "start_time": "2022-04-19T19:07:11.240Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Review remaining datatypes\n",
    "\n",
    "df_new_idx.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.566392Z",
     "start_time": "2022-04-19T19:07:11.243Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Recast DT\n",
    "\n",
    "for col in df_new_idx.columns:\n",
    "    df_new_idx[col] = pd.to_numeric(df_new_idx[col], downcast = 'float')\n",
    "\n",
    "df_new_idx.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.567360Z",
     "start_time": "2022-04-19T19:07:11.246Z"
    }
   },
   "outputs": [],
   "source": [
    "## Inspect results\n",
    "\n",
    "df_new_idx.T.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.568356Z",
     "start_time": "2022-04-19T19:07:11.251Z"
    }
   },
   "outputs": [],
   "source": [
    "## Create new total column\n",
    "\n",
    "df_new_idx['Total'] = df_new_idx.sum(axis=1)\n",
    "df_new_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.569355Z",
     "start_time": "2022-04-19T19:07:11.254Z"
    }
   },
   "outputs": [],
   "source": [
    "## Filter out zero sums\n",
    "\n",
    "df_compressed = df_new_idx[df_new_idx['Total'] >0]\n",
    "df_compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.569355Z",
     "start_time": "2022-04-19T19:07:11.256Z"
    }
   },
   "outputs": [],
   "source": [
    "df_compressed['Lounge']['Food']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.570383Z",
     "start_time": "2022-04-19T19:07:11.259Z"
    }
   },
   "outputs": [],
   "source": [
    "## Test print statements for logging purposes\n",
    "\n",
    "for col in df_compressed.columns[:-1]:\n",
    "    print(f\"Post to {col} food: {df_compressed[col]['Food']+df_compressed[col]['Other']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:07:17.572346Z",
     "start_time": "2022-04-19T19:07:11.262Z"
    }
   },
   "outputs": [],
   "source": [
    "## Further testing for logging purposes\n",
    "\n",
    "dept_list = [\"Rain 903\", \"In Room Dining\", \"Coffee Corner\", \"Lobby Bar\"]\n",
    "sd_list = [\"Food\", \"Liquor\", \"Tax\", \"Gratuity\"]\n",
    "\n",
    "for dept in dept_list:\n",
    "    for sd in sd_list:\n",
    "        print(dept+\" - \"+sd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbnds",
   "language": "python",
   "name": "sbnds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "781px",
    "left": "259px",
    "top": "314.125px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
